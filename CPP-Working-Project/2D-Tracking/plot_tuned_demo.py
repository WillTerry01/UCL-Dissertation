#!/usr/bin/env python3
"""
Visualization script for tuned parameter demonstration results.
Reads HDF5 files generated by demo_tuned_parameters.cpp and creates plots.
"""

import h5py
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.gridspec import GridSpec
import os

def load_demo_results(filename):
    """Load demonstration results from HDF5 file."""
    print(f"Loading results from: {filename}")
    
    if not os.path.exists(filename):
        print(f"Error: File {filename} not found!")
        print("Please run './demo_tuned_parameters' first to generate the data.")
        return None
    
    try:
        with h5py.File(filename, 'r') as f:
            # Load datasets
            true_states = np.array(f['true_states'])
            estimated_states = np.array(f['estimated_states'])
            measurements = np.array(f['measurements'])
            chi2_values = np.array(f['chi2_values'])
            
            # Load metadata
            tuned_q = f.attrs['tuned_q']
            tuned_R = f.attrs['tuned_R']
            dt = f.attrs['dt']
            best_cnees = f.attrs['best_cnees']
            
            print(f"Loaded {true_states.shape[0]} trajectories, each with {true_states.shape[1]} time steps")
            print(f"Tuned parameters: q={tuned_q:.4f}, R={tuned_R:.4f}, CNEES={best_cnees:.6f}")
            
            return {
                'true_states': true_states,
                'estimated_states': estimated_states,
                'measurements': measurements,
                'chi2_values': chi2_values,
                'tuned_q': tuned_q,
                'tuned_R': tuned_R,
                'dt': dt,
                'best_cnees': best_cnees
            }
    except Exception as e:
        print(f"Error loading file: {e}")
        return None

def plot_trajectory_comparison(data, run_idx=0):
    """Plot trajectory comparison for a specific run."""
    true_pos = data['true_states'][run_idx, :, :2]  # [x, y] positions
    est_pos = data['estimated_states'][run_idx, :, :2]
    measurements = data['measurements'][run_idx, :, :2]
    chi2 = data['chi2_values'][run_idx]
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Plot 1: Trajectory paths
    ax1 = axes[0]
    ax1.plot(true_pos[:, 0], true_pos[:, 1], 'g-', linewidth=2, label='True Trajectory', alpha=0.8)
    ax1.plot(est_pos[:, 0], est_pos[:, 1], 'b--', linewidth=2, label='Estimated Trajectory', alpha=0.8)
    ax1.scatter(measurements[:, 0], measurements[:, 1], c='red', s=20, alpha=0.6, label='Measurements', zorder=5)
    
    # Mark start and end points
    ax1.scatter(true_pos[0, 0], true_pos[0, 1], c='green', s=100, marker='s', label='Start (True)', zorder=10)
    ax1.scatter(true_pos[-1, 0], true_pos[-1, 1], c='green', s=100, marker='^', label='End (True)', zorder=10)
    ax1.scatter(est_pos[0, 0], est_pos[0, 1], c='blue', s=80, marker='o', label='Start (Est)', zorder=10)
    ax1.scatter(est_pos[-1, 0], est_pos[-1, 1], c='blue', s=80, marker='D', label='End (Est)', zorder=10)
    
    ax1.set_xlabel('X Position')
    ax1.set_ylabel('Y Position')
    ax1.set_title(f'Trajectory Comparison (Run {run_idx+1})\nChi² = {chi2:.2f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.axis('equal')
    
    # Plot 2: Position error over time
    ax2 = axes[1]
    time_steps = np.arange(len(true_pos)) * data['dt']
    pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
    
    ax2.plot(time_steps, pos_errors, 'r-', linewidth=2, label='Position Error')
    ax2.fill_between(time_steps, 0, pos_errors, alpha=0.3, color='red')
    
    mean_error = np.mean(pos_errors)
    ax2.axhline(y=mean_error, color='orange', linestyle='--', linewidth=2, 
                label=f'Mean Error = {mean_error:.3f}')
    
    ax2.set_xlabel('Time (s)')
    ax2.set_ylabel('Position Error')
    ax2.set_title('Position Error Over Time')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_velocity_comparison(data, run_idx=0):
    """Plot velocity comparison for a specific run."""
    true_vel = data['true_states'][run_idx, :, 2:]  # [vx, vy] velocities
    est_vel = data['estimated_states'][run_idx, :, 2:]
    time_steps = np.arange(len(true_vel)) * data['dt']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # X velocity
    axes[0, 0].plot(time_steps, true_vel[:, 0], 'g-', linewidth=2, label='True Vx')
    axes[0, 0].plot(time_steps, est_vel[:, 0], 'b--', linewidth=2, label='Estimated Vx')
    axes[0, 0].set_xlabel('Time (s)')
    axes[0, 0].set_ylabel('X Velocity')
    axes[0, 0].set_title('X Velocity Comparison')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Y velocity
    axes[0, 1].plot(time_steps, true_vel[:, 1], 'g-', linewidth=2, label='True Vy')
    axes[0, 1].plot(time_steps, est_vel[:, 1], 'b--', linewidth=2, label='Estimated Vy')
    axes[0, 1].set_xlabel('Time (s)')
    axes[0, 1].set_ylabel('Y Velocity')
    axes[0, 1].set_title('Y Velocity Comparison')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Velocity errors
    vel_errors = np.linalg.norm(true_vel - est_vel, axis=1)
    axes[1, 0].plot(time_steps, vel_errors, 'r-', linewidth=2)
    axes[1, 0].fill_between(time_steps, 0, vel_errors, alpha=0.3, color='red')
    mean_vel_error = np.mean(vel_errors)
    axes[1, 0].axhline(y=mean_vel_error, color='orange', linestyle='--', 
                       label=f'Mean = {mean_vel_error:.3f}')
    axes[1, 0].set_xlabel('Time (s)')
    axes[1, 0].set_ylabel('Velocity Error')
    axes[1, 0].set_title('Velocity Error Over Time')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Velocity components error
    vx_error = np.abs(true_vel[:, 0] - est_vel[:, 0])
    vy_error = np.abs(true_vel[:, 1] - est_vel[:, 1])
    axes[1, 1].plot(time_steps, vx_error, 'b-', linewidth=2, label='|Vx Error|')
    axes[1, 1].plot(time_steps, vy_error, 'r-', linewidth=2, label='|Vy Error|')
    axes[1, 1].set_xlabel('Time (s)')
    axes[1, 1].set_ylabel('Velocity Component Error')
    axes[1, 1].set_title('Velocity Component Errors')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_summary_statistics(data):
    """Plot summary statistics across all runs."""
    num_runs, num_steps, _ = data['true_states'].shape
    
    # Calculate statistics for each run
    pos_errors_all = []
    vel_errors_all = []
    final_pos_errors = []
    
    for run in range(num_runs):
        true_pos = data['true_states'][run, :, :2]
        est_pos = data['estimated_states'][run, :, :2]
        true_vel = data['true_states'][run, :, 2:]
        est_vel = data['estimated_states'][run, :, 2:]
        
        pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
        vel_errors = np.linalg.norm(true_vel - est_vel, axis=1)
        
        pos_errors_all.append(pos_errors)
        vel_errors_all.append(vel_errors)
        final_pos_errors.append(pos_errors[-1])
    
    pos_errors_all = np.array(pos_errors_all)
    vel_errors_all = np.array(vel_errors_all)
    
    fig = plt.figure(figsize=(16, 12))
    gs = GridSpec(3, 2, figure=fig)
    
    # Plot 1: Position errors over time (all runs)
    ax1 = fig.add_subplot(gs[0, :])
    time_steps = np.arange(num_steps) * data['dt']
    
    for run in range(num_runs):
        ax1.plot(time_steps, pos_errors_all[run], alpha=0.6, linewidth=1)
    
    mean_pos_errors = np.mean(pos_errors_all, axis=0)
    std_pos_errors = np.std(pos_errors_all, axis=0)
    ax1.plot(time_steps, mean_pos_errors, 'k-', linewidth=3, label='Mean')
    ax1.fill_between(time_steps, 
                     mean_pos_errors - std_pos_errors,
                     mean_pos_errors + std_pos_errors,
                     alpha=0.3, color='gray', label='±1σ')
    
    ax1.set_xlabel('Time (s)')
    ax1.set_ylabel('Position Error')
    ax1.set_title(f'Position Errors Across {num_runs} Runs\n' + 
                  f'Tuned Parameters: q={data["tuned_q"]:.4f}, R={data["tuned_R"]:.4f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Chi-squared values
    ax2 = fig.add_subplot(gs[1, 0])
    runs = np.arange(1, num_runs + 1)
    bars = ax2.bar(runs, data['chi2_values'], alpha=0.7, color='skyblue', edgecolor='navy')
    ax2.axhline(y=np.mean(data['chi2_values']), color='red', linestyle='--', 
                linewidth=2, label=f'Mean = {np.mean(data["chi2_values"]):.2f}')
    ax2.set_xlabel('Run Number')
    ax2.set_ylabel('Chi-squared Value')
    ax2.set_title('Chi-squared Values by Run')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Final position errors
    ax3 = fig.add_subplot(gs[1, 1])
    ax3.bar(runs, final_pos_errors, alpha=0.7, color='lightcoral', edgecolor='darkred')
    ax3.axhline(y=np.mean(final_pos_errors), color='blue', linestyle='--', 
                linewidth=2, label=f'Mean = {np.mean(final_pos_errors):.3f}')
    ax3.set_xlabel('Run Number')
    ax3.set_ylabel('Final Position Error')
    ax3.set_title('Final Position Errors by Run')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Error distribution histogram
    ax4 = fig.add_subplot(gs[2, :])
    all_pos_errors = pos_errors_all.flatten()
    ax4.hist(all_pos_errors, bins=50, alpha=0.7, color='lightgreen', edgecolor='darkgreen')
    ax4.axvline(x=np.mean(all_pos_errors), color='red', linestyle='--', 
                linewidth=2, label=f'Mean = {np.mean(all_pos_errors):.3f}')
    ax4.axvline(x=np.median(all_pos_errors), color='blue', linestyle='--', 
                linewidth=2, label=f'Median = {np.median(all_pos_errors):.3f}')
    ax4.set_xlabel('Position Error')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Distribution of All Position Errors')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def main():
    """Main function to generate all plots."""
    print("=== Tuned Parameter Visualization ===")
    
    # Load data
    data_file = "../2D-Tracking/Saved_Data/2D_tuned_demo_results.h5"
    data = load_demo_results(data_file)
    
    if data is None:
        return
    
    print(f"\nGenerating plots for {data['true_states'].shape[0]} trajectories...")
    
    # Create output directory for plots
    os.makedirs("plots", exist_ok=True)
    
    # Plot 1: Trajectory comparison for first run
    print("Creating trajectory comparison plot...")
    fig1 = plot_trajectory_comparison(data, run_idx=0)
    fig1.savefig("../2D-Tracking/plots/trajectory_comparison.png", dpi=300, bbox_inches='tight')
    plt.close(fig1)
    
    # Plot 2: Velocity comparison for first run
    print("Creating velocity comparison plot...")
    fig2 = plot_velocity_comparison(data, run_idx=0)
    fig2.savefig("../2D-Tracking/plots/velocity_comparison.png", dpi=300, bbox_inches='tight')
    plt.close(fig2)
    
    # Plot 3: Summary statistics
    print("Creating summary statistics plot...")
    fig3 = plot_summary_statistics(data)
    fig3.savefig("../2D-Tracking/plots/summary_statistics.png", dpi=300, bbox_inches='tight')
    plt.close(fig3)
    
    # Generate additional trajectory comparisons for multiple runs
    num_runs = min(3, data['true_states'].shape[0])
    for run_idx in range(num_runs):
        print(f"Creating trajectory plot for run {run_idx + 1}...")
        fig = plot_trajectory_comparison(data, run_idx=run_idx)
        fig.savefig(f"../2D-Tracking/plots/trajectory_run_{run_idx+1}.png", dpi=300, bbox_inches='tight')
        plt.close(fig)
    
    print("\n=== Plotting Complete ===")
    print("Generated plots saved in '../2D-Tracking/plots/' directory:")
    print("  - trajectory_comparison.png: Main trajectory comparison")
    print("  - velocity_comparison.png: Velocity analysis")
    print("  - summary_statistics.png: Overall performance statistics")
    print(f"  - trajectory_run_*.png: Individual run comparisons")
    print("\nSummary of tuned parameters:")
    print(f"  Process noise intensity (q): {data['tuned_q']:.4f}")
    print(f"  Measurement noise variance (R): {data['tuned_R']:.4f}")
    print(f"  Best CNEES objective: {data['best_cnees']:.6f}")

if __name__ == "__main__":
    main() 