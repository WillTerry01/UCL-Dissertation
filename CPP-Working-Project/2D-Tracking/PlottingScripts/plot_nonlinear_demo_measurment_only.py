#!/usr/bin/env python3
"""
Visualization script for pure measurement tracking demonstration results.
Reads HDF5 files generated by demo_nonlinear_pure_measurement.cpp and creates plots
showing how the system converges from uncertain initialization to final estimates.
"""

import h5py
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.gridspec import GridSpec
import os

def load_pure_measurement_results(filename):
    """Load pure measurement demonstration results from HDF5 file."""
    print(f"Loading pure measurement results from: {filename}")
    
    if not os.path.exists(filename):
        print(f"Error: File {filename} not found!")
        print("Please run './demo_nonlinear_pure_measurement' first to generate the data.")
        return None
    
    try:
        with h5py.File(filename, 'r') as f:
            # Load datasets
            true_states = np.array(f['true_states'])
            estimated_states = np.array(f['estimates'])
            initial_estimates = np.array(f['initial_estimates'])
            measurements = np.array(f['measurements'])
            chi2_values = np.array(f['chi2_values'])
            consistency_metrics = np.array(f['consistency_metrics'])
            
            print(f"Loaded {true_states.shape[0]} pure measurement trajectories, each with {true_states.shape[1]} time steps")
            print(f"Chi² range: {np.min(chi2_values):.2f} to {np.max(chi2_values):.2f}")
            print(f"Consistency metrics range: {np.min(consistency_metrics):.3f} to {np.max(consistency_metrics):.3f}")
            
            return {
                'true_states': true_states,
                'estimated_states': estimated_states,
                'initial_estimates': initial_estimates,
                'measurements': measurements,
                'chi2_values': chi2_values,
                'consistency_metrics': consistency_metrics
            }
    except Exception as e:
        print(f"Error loading file: {e}")
        return None

def plot_convergence_analysis(data, run_idx=0):
    """Plot convergence analysis showing initial uncertainty to final convergence."""
    true_pos = data['true_states'][run_idx, :, :2]  # [x, y] positions
    est_pos = data['estimated_states'][run_idx, :, :2]
    init_pos = data['initial_estimates'][run_idx, :, :2]
    measurements = data['measurements'][run_idx, :, :2]
    chi2 = data['chi2_values'][run_idx]
    consistency = data['consistency_metrics'][run_idx]
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Plot 1: Trajectory comparison with initialization uncertainty
    ax1 = axes[0, 0]
    ax1.plot(true_pos[:, 0], true_pos[:, 1], 'g-', linewidth=3, label='True Trajectory', alpha=0.8)
    ax1.plot(est_pos[:, 0], est_pos[:, 1], 'b--', linewidth=3, label='Final Estimates', alpha=0.8)
    ax1.plot(init_pos[:, 0], init_pos[:, 1], 'r:', linewidth=2, label='Initial Estimates', alpha=0.6)
    ax1.scatter(measurements[:, 0], measurements[:, 1], c='orange', s=30, alpha=0.7, label='GPS Measurements', zorder=5)
    
    # Mark start and end points
    ax1.scatter(true_pos[0, 0], true_pos[0, 1], c='green', s=150, marker='s', label='Start (True)', zorder=10)
    ax1.scatter(true_pos[-1, 0], true_pos[-1, 1], c='green', s=150, marker='^', label='End (True)', zorder=10)
    ax1.scatter(est_pos[0, 0], est_pos[0, 1], c='blue', s=120, marker='o', label='Start (Est)', zorder=10)
    ax1.scatter(est_pos[-1, 0], est_pos[-1, 1], c='blue', s=120, marker='D', label='End (Est)', zorder=10)
    ax1.scatter(init_pos[0, 0], init_pos[0, 1], c='red', s=100, marker='x', label='Start (Init)', zorder=10)
    
    ax1.set_xlabel('X Position (m)')
    ax1.set_ylabel('Y Position (m)')
    ax1.set_title(f'Pure Measurement Tracking Convergence (Run {run_idx+1})\n' + 
                  f'Chi² = {chi2:.2f}, Consistency = {consistency:.3f}')
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax1.grid(True, alpha=0.3)
    ax1.axis('equal')
    
    # Plot 2: Position error convergence over time
    ax2 = axes[0, 1]
    time_steps = np.arange(len(true_pos)) * 0.1  # Assuming dt = 0.1s
    pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
    init_errors = np.linalg.norm(true_pos - init_pos, axis=1)
    
    ax2.plot(time_steps, pos_errors, 'b-', linewidth=2, label='Final Position Error')
    ax2.plot(time_steps, init_errors, 'r:', linewidth=2, label='Initial Position Error')
    ax2.fill_between(time_steps, 0, pos_errors, alpha=0.3, color='blue')
    ax2.fill_between(time_steps, 0, init_errors, alpha=0.2, color='red')
    
    mean_error = np.mean(pos_errors)
    ax2.axhline(y=mean_error, color='blue', linestyle='--', linewidth=2, 
                label=f'Mean Final Error = {mean_error:.3f} m')
    
    ax2.set_xlabel('Time (s)')
    ax2.set_ylabel('Position Error (m)')
    ax2.set_title('Position Error Convergence Over Time')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Velocity comparison
    ax3 = axes[1, 0]
    true_vel = data['true_states'][run_idx, :, 2:]  # [vx, vy] velocities
    est_vel = data['estimated_states'][run_idx, :, 2:]
    init_vel = data['initial_estimates'][run_idx, :, 2:]
    
    ax3.plot(time_steps, true_vel[:, 0], 'g-', linewidth=2, label='True Vx')
    ax3.plot(time_steps, est_vel[:, 0], 'b--', linewidth=2, label='Estimated Vx')
    ax3.plot(time_steps, init_vel[:, 0], 'r:', linewidth=1, label='Initial Vx', alpha=0.6)
    ax3.plot(time_steps, true_vel[:, 1], 'g-', linewidth=2, label='True Vy')
    ax3.plot(time_steps, est_vel[:, 1], 'b--', linewidth=2, label='Estimated Vy')
    ax3.plot(time_steps, init_vel[:, 1], 'r:', linewidth=1, label='Initial Vy', alpha=0.6)
    
    ax3.set_xlabel('Time (s)')
    ax3.set_ylabel('Velocity (m/s)')
    ax3.set_title('Velocity Estimation Convergence')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Error convergence analysis
    ax4 = axes[1, 1]
    vel_errors = np.linalg.norm(true_vel - est_vel, axis=1)
    init_vel_errors = np.linalg.norm(true_vel - init_vel, axis=1)
    
    ax4.plot(time_steps, pos_errors, 'b-', linewidth=2, label='Position Error')
    ax4.plot(time_steps, vel_errors, 'r-', linewidth=2, label='Velocity Error')
    ax4.plot(time_steps, init_errors, 'b:', linewidth=1, label='Initial Position Error', alpha=0.6)
    ax4.plot(time_steps, init_vel_errors, 'r:', linewidth=1, label='Initial Velocity Error', alpha=0.6)
    
    ax4.set_xlabel('Time (s)')
    ax4.set_ylabel('Error (m or m/s)')
    ax4.set_title('Error Convergence Analysis')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_initialization_uncertainty(data, run_idx=0):
    """Plot the initial uncertainty and how it affects tracking."""
    true_pos = data['true_states'][run_idx, :, :2]
    est_pos = data['estimated_states'][run_idx, :, :2]
    init_pos = data['initial_estimates'][run_idx, :, :2]
    measurements = data['measurements'][run_idx, :, :2]
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Plot 1: Initial uncertainty visualization
    ax1 = axes[0]
    
    # Plot trajectories
    ax1.plot(true_pos[:, 0], true_pos[:, 1], 'g-', linewidth=3, label='True Trajectory', alpha=0.8)
    ax1.plot(est_pos[:, 0], est_pos[:, 1], 'b--', linewidth=3, label='Final Estimates', alpha=0.8)
    ax1.plot(init_pos[:, 0], init_pos[:, 1], 'r:', linewidth=2, label='Initial Estimates', alpha=0.6)
    
    # Show initial uncertainty region
    init_uncertainty = 2.0  # ±2m as set in the demo
    circle = patches.Circle((true_pos[0, 0], true_pos[0, 1]), init_uncertainty, 
                           fill=False, color='red', linestyle='--', linewidth=2, 
                           label=f'Initial Uncertainty (±{init_uncertainty}m)')
    ax1.add_patch(circle)
    
    # Mark key points
    ax1.scatter(true_pos[0, 0], true_pos[0, 1], c='green', s=150, marker='s', label='True Start', zorder=10)
    ax1.scatter(init_pos[0, 0], init_pos[0, 1], c='red', s=120, marker='x', label='Estimated Start', zorder=10)
    ax1.scatter(measurements[0, 0], measurements[0, 1], c='orange', s=100, marker='o', label='First GPS', zorder=10)
    
    ax1.set_xlabel('X Position (m)')
    ax1.set_ylabel('Y Position (m)')
    ax1.set_title(f'Initial Uncertainty Analysis (Run {run_idx+1})')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.axis('equal')
    
    # Plot 2: Error reduction over time
    ax2 = axes[1]
    time_steps = np.arange(len(true_pos)) * 0.1
    
    pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
    init_errors = np.linalg.norm(true_pos - init_pos, axis=1)
    
    # Calculate improvement
    improvement = init_errors - pos_errors
    
    ax2.plot(time_steps, pos_errors, 'b-', linewidth=2, label='Final Position Error')
    ax2.plot(time_steps, init_errors, 'r:', linewidth=2, label='Initial Position Error')
    ax2.fill_between(time_steps, pos_errors, init_errors, alpha=0.3, color='green', 
                     label='Improvement Region')
    
    ax2.set_xlabel('Time (s)')
    ax2.set_ylabel('Position Error (m)')
    ax2.set_title('Tracking Improvement Over Time')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_performance_summary(data):
    """Plot overall performance summary across all runs."""
    num_runs, num_steps, _ = data['true_states'].shape
    
    # Calculate statistics for each run
    initial_errors = []
    final_errors = []
    convergence_rates = []
    
    for run in range(num_runs):
        true_pos = data['true_states'][run, :, :2]
        est_pos = data['estimated_states'][run, :, :2]
        init_pos = data['initial_estimates'][run, :, :2]
        
        # Initial and final errors
        init_error = np.linalg.norm(true_pos[0] - init_pos[0])
        final_error = np.linalg.norm(true_pos[-1] - est_pos[-1])
        
        initial_errors.append(init_error)
        final_errors.append(final_error)
        
        # Convergence rate (how quickly error reduces)
        pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
        convergence_rate = (init_error - np.mean(pos_errors)) / init_error
        convergence_rates.append(convergence_rate)
    
    fig = plt.figure(figsize=(16, 10))
    gs = GridSpec(2, 3, figure=fig)
    
    # Plot 1: Initial vs Final errors
    ax1 = fig.add_subplot(gs[0, 0])
    runs = np.arange(1, num_runs + 1)
    x = np.arange(len(runs))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, initial_errors, width, label='Initial Error', color='red', alpha=0.7)
    bars2 = ax1.bar(x + width/2, final_errors, width, label='Final Error', color='blue', alpha=0.7)
    
    ax1.set_xlabel('Run Number')
    ax1.set_ylabel('Position Error (m)')
    ax1.set_title('Initial vs Final Position Errors')
    ax1.set_xticks(x)
    ax1.set_xticklabels(runs)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Convergence rates
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.bar(runs, convergence_rates, alpha=0.7, color='green', edgecolor='darkgreen')
    ax2.axhline(y=np.mean(convergence_rates), color='red', linestyle='--', 
                linewidth=2, label=f'Mean = {np.mean(convergence_rates):.2f}')
    ax2.set_xlabel('Run Number')
    ax2.set_ylabel('Convergence Rate')
    ax2.set_title('Tracking Convergence Rates\n(Higher = Better)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Consistency metrics
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.bar(runs, data['consistency_metrics'], alpha=0.7, color='orange', edgecolor='darkorange')
    ax3.axhline(y=1.0, color='green', linestyle='-', linewidth=2, 
                label='Optimal = 1.0', alpha=0.7)
    ax3.axhline(y=np.mean(data['consistency_metrics']), color='red', linestyle='--', 
                linewidth=2, label=f'Mean = {np.mean(data["consistency_metrics"]):.3f}')
    ax3.set_xlabel('Run Number')
    ax3.set_ylabel('Consistency Metric')
    ax3.set_title('Consistency Metrics\n(≈1.0 = Optimal Parameters)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Error reduction over time (all runs)
    ax4 = fig.add_subplot(gs[1, :])
    time_steps = np.arange(num_steps) * 0.1
    
    for run in range(num_runs):
        true_pos = data['true_states'][run, :, :2]
        est_pos = data['estimated_states'][run, :, :2]
        init_pos = data['initial_estimates'][run, :, :2]
        
        pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
        init_errors_run = np.linalg.norm(true_pos - init_pos, axis=1)
        
        ax4.plot(time_steps, pos_errors, alpha=0.6, linewidth=1, label=f'Run {run+1}' if run == 0 else "")
        ax4.plot(time_steps, init_errors_run, alpha=0.3, linewidth=1, linestyle=':', 
                color='red' if run == 0 else 'gray')
    
    # Plot mean convergence
    all_pos_errors = []
    all_init_errors = []
    for run in range(num_runs):
        true_pos = data['true_states'][run, :, :2]
        est_pos = data['estimated_states'][run, :, :2]
        init_pos = data['initial_estimates'][run, :, :2]
        
        pos_errors = np.linalg.norm(true_pos - est_pos, axis=1)
        init_errors_run = np.linalg.norm(true_pos - init_pos, axis=1)
        
        all_pos_errors.append(pos_errors)
        all_init_errors.append(init_errors_run)
    
    mean_pos_errors = np.mean(all_pos_errors, axis=0)
    mean_init_errors = np.mean(all_init_errors, axis=0)
    
    ax4.plot(time_steps, mean_pos_errors, 'k-', linewidth=3, label='Mean Final Error')
    ax4.plot(time_steps, mean_init_errors, 'r:', linewidth=2, label='Mean Initial Error')
    
    ax4.set_xlabel('Time (s)')
    ax4.set_ylabel('Position Error (m)')
    ax4.set_title(f'Error Convergence Across {num_runs} Runs\n' + 
                  f'Mean Initial Error: {np.mean(initial_errors):.2f}m → Final: {np.mean(final_errors):.2f}m')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def main():
    """Main function to generate all pure measurement plots."""
    print("=== Pure Measurement Tracking Visualization ===")
    
    # Load data
    data_file = "../2D-Tracking/Saved_Data/nonlinear_pure_measurement_results.h5"
    data = load_pure_measurement_results(data_file)
    
    if data is None:
        return
    
    print(f"\nGenerating plots for {data['true_states'].shape[0]} pure measurement trajectories...")
    
    # Create output directory for plots
    plots_dir = os.path.join(os.path.dirname(__file__), "..", "plots")
    os.makedirs(plots_dir, exist_ok=True)
    print(f"Plots will be saved to: {plots_dir}")
    
    # Plot 1: Convergence analysis for first run
    print("Creating convergence analysis plot...")
    fig1 = plot_convergence_analysis(data, run_idx=0)
    fig1.savefig(os.path.join(plots_dir, "pure_measurement_convergence.png"), dpi=300, bbox_inches='tight')
    plt.close(fig1)
    
    # Plot 2: Initialization uncertainty analysis
    print("Creating initialization uncertainty plot...")
    fig2 = plot_initialization_uncertainty(data, run_idx=0)
    fig2.savefig(os.path.join(plots_dir, "pure_measurement_initialization.png"), dpi=300, bbox_inches='tight')
    plt.close(fig2)
    
    # Plot 3: Performance summary
    print("Creating performance summary plot...")
    fig3 = plot_performance_summary(data)
    fig3.savefig(os.path.join(plots_dir, "pure_measurement_performance.png"), dpi=300, bbox_inches='tight')
    plt.close(fig3)
    
    # Generate additional convergence plots for multiple runs
    num_runs = min(3, data['true_states'].shape[0])
    for run_idx in range(num_runs):
        print(f"Creating convergence plot for run {run_idx + 1}...")
        fig = plot_convergence_analysis(data, run_idx=run_idx)
        fig.savefig(os.path.join(plots_dir, f"pure_measurement_convergence_run_{run_idx+1}.png"), dpi=300, bbox_inches='tight')
        plt.close(fig)
    
    print("\n=== Pure Measurement Plotting Complete ===")
    print(f"Generated plots saved in: {plots_dir}")
    print("  - pure_measurement_convergence.png: Main convergence analysis")
    print("  - pure_measurement_initialization.png: Initial uncertainty analysis")
    print("  - pure_measurement_performance.png: Overall performance summary")
    print(f"  - pure_measurement_convergence_run_*.png: Individual run convergence")
    
    print("\nSummary of pure measurement tracking performance:")
    print(f"  Number of runs: {len(data['chi2_values'])}")
    print(f"  Average chi-squared: {np.mean(data['chi2_values']):.2f}")
    print(f"  Average consistency metric: {np.mean(data['consistency_metrics']):.3f}")
    print(f"  Consistency metric range: {np.min(data['consistency_metrics']):.3f} to {np.max(data['consistency_metrics']):.3f}")
    
    # Calculate convergence statistics
    initial_errors = []
    final_errors = []
    for run in range(len(data['chi2_values'])):
        true_pos = data['true_states'][run, :, :2]
        est_pos = data['estimated_states'][run, :, :2]
        init_pos = data['initial_estimates'][run, :, :2]
        
        init_error = np.linalg.norm(true_pos[0] - init_pos[0])
        final_error = np.linalg.norm(true_pos[-1] - est_pos[-1])
        
        initial_errors.append(init_error)
        final_errors.append(final_error)
    
    print(f"  Average initial position error: {np.mean(initial_errors):.2f} m")
    print(f"  Average final position error: {np.mean(final_errors):.2f} m")
    print(f"  Average improvement: {np.mean(initial_errors) - np.mean(final_errors):.2f} m")
    
    # Parameter quality assessment
    consistency = data['consistency_metrics']
    optimal_count = np.sum(np.abs(consistency - 1.0) < 0.2)
    print(f"  Runs with optimal parameters (consistency ≈ 1.0): {optimal_count}/{len(consistency)}")

if __name__ == "__main__":
    main() 